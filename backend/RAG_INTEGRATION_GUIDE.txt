# Add this import at the top of chat.py (line 7)
from services import gemini_client, finance_logic, pathway_rag

# Replace the agent pipeline function starting at line 133 with this enhanced version:

        # 2️⃣ Compute Pathway-based metrics
        analysis = finance_logic.analyze_financials(structured_data)
        metrics = analysis.get("summary", {})
        insights = analysis.get("insights", [])

        update_status(chat_id, "retrieving_context", 60, "Searching documents with RAG")

        # 2.5️⃣ Get RAG context for the query  
        try:
            rag_context = pathway_rag.get_rag_context(task_id, user_message)
            context_text = rag_context.get("context", "")
            sources = rag_context.get("sources", [])
            
            # Format citations for response
            citations = []
            for source in sources:
                citations.append({
                    "doc": source.get("filename", "Unknown"),
                    "page": source.get("chunk_index", 0) + 1
                })
        except Exception as e:
            print(f"⚠️ RAG retrieval failed (non-critical): {e}")
            context_text = ""
            citations = []

        update_status(chat_id, "summarizing", 75, "Generating CFO summary via Gemini")

        # 3️⃣ Create LLM summary using Gemini with RAG context
        system_prompt = (
            "You are a CFO assistant. Given structured financial data, computed ratios, "
            "and relevant document excerpts, write a concise but insightful summary. "
            "Highlight key risks, leverage, liquidity, and performance insights. "
            "When referencing specific information, cite the document sources provided."
        )
        
        user_prompt = (
            f"User question: {user_message}\n\n"
            f"Structured data: {json.dumps(structured_data, indent=2)}\n\n"
            f"Computed metrics: {json.dumps(metrics, indent=2)}\n\n"
            f"Insights: {json.dumps(insights, indent=2)}\n\n"
        )
        
        if context_text:
            user_prompt += f"Relevant document excerpts:\n{context_text}\n\n"
        
        user_prompt += "Provide a comprehensive answer using both the structured metrics and document context."
        
        prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
        summary = gemini_client.ask_gemini(prompt)

        update_status(chat_id, "saving_results", 90, "Saving CFO agent results")

        # 4️⃣ Save to DB with citations
        chat_msg = session.get(ChatMessage, chat_id)
        chat_msg.role = "agent"
        chat_msg.content = summary
        chat_msg.reasoning_log = json.dumps(insights)
        chat_msg.citations = json.dumps(citations)  # Now includes RAG sources
        chat_msg.status = "done"
        session.add(chat_msg)
